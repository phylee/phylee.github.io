---
title: 我的涂鸦三年
date: 2022-05-03 16:40:14
tags:
- Work
---

时间过的很快，我在涂鸦已经工作三年了。三年不算长，但也是我大学毕业至今很重要的三年，也是我成长最快的三年，所以也该写一个总结。回首过往，总结经验，是为了更好的面对未来挑战。下面的回顾文字，可能会有马后炮的嫌疑（当然任何回顾总结都有这种嫌疑），只作为个人工作的一小段总结，不作其他的引申。

## 2019年：打扫干净屋子

第一年刚入职不久，公司老员工相继离职，自己也不是很熟悉现有平台的开发语言（我原来写Python，现有平台后端是Java，前端是Veujs），平台功能很全但也很复杂，同时也有些不稳定，挑战对我来说是前所未有的，所以基本上也没有休息过一个完整的周末。

首先就是要保证平台的稳定使用。要保证稳定首先就需要化繁为简，复杂性是软件工程上的万恶之源，因为人天生就不愿意处理很复杂的东西，更别说还需要长时间的维护；其次要聚焦平台的主航道，人的精力有限无法面面具到的情况下，要优先保证平台的核心功能稳定可靠，其他次要功能能砍则砍，不能砍的也不投入过多时间，让其自生自灭。

主要做的工作如下：

> 1. 推：优先级较低的新功能需求推掉或延期支持，先保证发布核心功能的稳定。
> 2. 砍：不重要的或者不需要的功能先砍掉。
>   例如：下线工单相关、vpn权限管理、机器权限管理、需求管理等等，同时清理删除大量毁弃的代码。新的运维平台支持工单功能，就将现有的工单下掉了。
> 需求管理这块本来是需要的也是重要的，对发布平台来说，从需求管理到上线是个完整的生命周期，但是人力有限，也忍痛砍掉了（后面公司成立新的效能团队，做了一个新的项目需求平台，再后来这个团队又解散了，当然这是后话）。
> 3. 迁：和发布平台核心不相关的功能迁移，专注自身的平台定位。
>  例如：日志报警，健康检查报警等迁移到监控平台，机器管理等相关迁移新的运维平台。
> 4. 优：故障频发的接口重点优化。使用频繁和问题多的接口能重构的重构，能优化的优化，维护成本高的难以优化的就弃用。
>  集中一段时间修复bug，当然有些也不能算bug，只是接口校验写的不够严谨。大量bug，会导致大量时间浪费在定位以及处理数据不一致的问题，无法开展新的功能开发。同时接口中增加一些防御性检查，避免误操作。前端输入增加长度限制，避免超过数据库字段限制。增加重试，减少网络问题影响。在和其他平台对接时，要避免其他平台异常导致自身平台无法使用的情况，可降级，避免强依赖（当然有些依赖是无法完全解耦的）。
> 关于bug，很多人觉得要做到平台零bug，发现bug就要立即修复完。我的看法是bug永远都修复不完的，也不可能投入所有精力去修复全部bug。只要有新的功能特性就会有新的bug，我们只要保证bug降低一定范围以下同时优先级足够低即可。要重视bug但是不能被bug绊住脚步。
> 5. 改：我们同时进行了一个大的改造，proxy重写为gateway。
>  这次大重写一方面是为了稳定性，另一方面也是为了减少我们的支撑时间。原有的发布过程中的日志不能实时显示给开发人员，这样开发人员就不能自助定位失败问题，这会就会频繁找我们，浪费我们大量的支撑时间，当然也会浪费开发人员的大量时间。同时此次重写将部署的脚本也统一收到平台管理，在部署时自动下发。极大地降低脚本的不一致性和升级成本，同时让我们也能灰度发布更新脚本。
> 此次重写也让我们发生了一个导致业务的线上故障。我的经验就是任何重构都有可能导致故障，只有重构收益大，我们就不能怕。事前充分准备尽量避免，事中要尽量可灰度去替换，事后要能做到快速回滚。
> 6. 减：减少处理“杂事”的时间。
>  一开始平台有大量的事情，要平台管理员处理，这会占用我们不少的时间，当然这也不符合我的理念，我们做的是平台，平台当然要像ATM机器一样能自助，除非出了故障才需要找我们处理。于是我们花费了一些精力将平台管理员的权限能转移给主管的转给主管，能转移给应用负责人的就转移给应用负责人。减少平台管理员花费大量时间在平台用户权限申请上。
> 理想情况下，平台的用户不需要找管理员要任何权限，因为我们做的平台应该是自助的，当然自助就要可能发生一些非主观的问题甚至故障，所以我们做好平台用户的使用培训，对一些危险操作要重复确认，多次校验，事后要有操作日志可追踪回溯。不能害怕用户犯错，就不放权，这样我们无法让平台更易用。
> 7. 易：让平台自身做到更容易的发布，减少部署时间花费。
>  我们平台发布的应用都是使用docker镜像部署，但是发布平台自身却不是，这个是不合适的，做平台首先要eat our own dog food。我们花了一点时间，各个应用都迁移使用docker镜像部署，同时使用gitlab的CI自动构建镜像，也节省了平台自身的部署时间。当然最好是平台自己能发布自己，要像某些编程语言那样具有自举的能力（因为一些平台自身的问题，我们当时没做到自己发布自己，后面做新平台时我们做到了，当然这是后话）。

这一年我们犯了一些错误，但是最重要的是让平台自身更加完善了，当然这些都离不开同事的协作，领导的支持。现在的工作要想做好一些事，一个人当然是不行的，更需要有一个优秀的同事团队。

## 2020年：再请客

经过上一年的不断完善，平台基本稳定了，同时平台的代码我们也熟悉了，新的一年我们要再出发。

这一年我们做了很多新的功能特性，增强并扩展了平台的持续交付的能力。

> 1. 拥抱云原生, 改造发布平台同时支持虚拟机部署和k8s部署
>  从19年底我们就开始初步试点k8s部署，试点没大问题后，在2020年我们开始了大规模的迁移。
> 因为原来虚拟机部署也是用的docker镜像作为制品进行发布的，所以这也让我们迁移k8s更容易，改造成本更低。最大的难度可能是分批部署，因为我们主要是云端的应用发布，能分批，小规模灰度很重要，但是k8s的原生工作负载Deployment不支持分批暂停，所以我们基于Deployment自研了一个CRD。
> 发布平台方面在设计的时候我们要考虑到迁移后如果有问题，如何能快速迁移回去？是按照一个应用整体迁移，还是按照环境进行迁移？当然我们公司在全球有多个数据中心，多个公有云厂商，不同云厂商的k8s支持程度不一样，还是按照区域去迁移？这些都是我们要考虑的细节。
> 2. 新增按项目维度的隔离测试环境，增强测试环境的稳定性和灵活性。
>  微服务越来越多，开发人员也越来越多，测试环境的稳定性就会遇到很大的挑战。需求越来越多，大家都要测试，但是测试环境就几个，大家都在里面部署，一个服务部署有问题，就可能导致很多依赖服务测试不了。于是今年我们对日常环境做了一些改造，让原来的日常环境只部署线上最新代码并且可以线上发布完自动部署，同时虚拟出一个测试的隔离环境，一个需求的相关应用部署在一个隔离环境，打上同一个标签，其实有点写时复制的思想，当你交付的一个需求要修改的那几个应用，就新建一个项目将相关应用关联起来，打上标签进行部署，不需要修改代码的但是测试需要依赖的应用就使用日常部署了master代码的主干环境。这样可以用最少的机器同时可以虚拟出无数个测试的隔离环境（因为都打上了标签，每个标签之间不会调用，看起来是隔离的）。当然这是需要中间件，前端，客户端，云端等多部门协作和支持这个标签，不单单靠一个发布平台就能搞定的。
> 3. 持续集成的相关资源改造：由单机走向集群。
>   随着应用和开发人员的增多，任务类型也由原来的只有编译打包扩展到单元测试，代码扫描等多种类型任务，原来只有一台构建机器，已经无法满足需求，所以我们需要进行改造。但是缺少个任务调度和资源管理的平台，人力有限不可能去开发一个，正好我们在迁移k8s, 所以利用k8s作为资源池是最好的方式，同时我们引入开源的Tekon作为的任务管理引擎。顺利的从单机迁移到k8s资源池。
> 其实这也是我一直以来想做的，就是从下载代码，编译打包，单元测试，构建镜像，部署等完整流程，都可以在一个k8s集群完成，这样我们可以充分利用k8s的能力。
>
> 4. 多架构镜像部署：降低机器成本。
>   ARM机型的价格相对便宜很多，但是我们是多云的，有些云厂商的ARM机器支持还不是很好，所以我们要同时支持ARM64和x86-64两种架构的编译和镜像构建。主要利用docker mainfest和buildx两种方式。本来计划大规模使用buildx的，但是这个工具稳定性还不是太好，所以只有小部分使用。主要还是用原生的docker manifest。
> 5. 研发质量提升
>  我们支持了单元测试，代码扫描，对接研测平台，对依赖包进行检测等等，平台虽然提供了基本的能力，但是使用的还不太理想，一方面是这需要开发人员有意愿提升代码质量我们不能强制，另一方面我们时间精力有限，所以这方面投入也比较少，没有持续优化。

## 2021年：另起炉灶

这一年因为组织架构调整，团队成员也增加了不少，希望整合原有的发布平台和运维平台，做一个新研发平台。减少开发人员的使用成本。

新平台做的很不容易，中间经历多次组织架构调整，项目差点死掉，后又经历人员的减少和拆分，各种软甜苦辣吧，花了近一年的时间新平台才勉强覆盖老平台的所有功能并开始迁移。当然最后的结果没有达到我的预期，但是中间没有死掉，也是不容易了，最重要的是也学到了很多经验教训。

> 1. 如何从零开始做一个项目？
> 设计文档，我们要做哪些功能，提供哪些能力？
> 需求整理，我们如何去迭代，先做哪些，后做哪些？
> 服务划分，我们要分几个应用，功能如何划分？
> 原型设计，前端如何展示和组合？
> 迭代开发，如何评估开发时间和人员分工?
> 开发完成，如何测试，如何上线？
> 2. 重新梳理的已有的所有功能，加深理解，重新优化。
>  原来的平台功能很多，但是都没有整理过，这次进行了梳理，对发布平台的功能有更深的理解，例如应用，项目，编译构建，部署，流水线等等。
> 3. 对当前平台架构的新思考：当前发布平台和运维平台职责不清，如何进行整合和并调整定位？
> 当然这个不是当前的平台架构，只是我在做的过程的一些新思考，只是一个新设想。
> 以发布平台为基础，整合扩展为新的研发平台，面向开发人员的一站式平台。也就是只要是开发人员需要的功能只在这一个平台里操作就行，从需求，开发，部署，上线，日志，监控数据反馈一体化。后端数据可以分开多个服务，前端要整合一个页面。
> 以运维平台为基础，整合扩展为多云的管理平台，专注于面向运维人员的一站式平台以及公司基础运维数据存储。首先将开发人员需要使用的功能迁移研发平台，避免开发人员多个平台跳转，也就是说这个平台不再对开发人员开放，只面向运维人员，这里的运维是广义的，包括业务运维，系统运维，DBA，k8s集群管理员等等。专注于封装各公有云，私有云的操作，尽量减少公司内运维人员到各云厂商控制台上的操作，只要在该平台上就可以操作所有云。后期这个平台可以独立部署私有云售卖，甚至开源。
> 4. 对重构平台的思考
> 尽量减少新平台从开发到对外使用的时间。长时间不对外使用，不可预知的风险就会很高。
> 可以分模块重构，尽量避免从零开始完全重做，因为这样会导致时间周期很长。
> 数据库除如果要独立使用，不和老平台复用，要多考虑中间过程数据一致性和迁移的时间成本。
> 尽量减少重构对平台用户和其他对接平台的影响。

## 反思

这几年我也做了很多在今天看来不是很好的决定，如果重新做一次决定，会不会更好？我想是不会的，这是我当时的能力和当时的外在条件限制决定的，不能马后炮，也不要用上帝视角，但是可以反思一下，以后做的更好一点。
